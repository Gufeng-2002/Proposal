\section{Literature Review}

\subsection{Sediment Contamination Assessment}

Sediment contamination assessment serves as a fundamental tool for understanding
and protecting aquatic ecosystem health, as contaminated sediments pose significant
risks to benthic communities and serve as long-term sources of pollutants 
that can affect entire food webs
\cite{MacDonald2000SedimentGuidelines, Burton2010MultipleStressors}.
Sediments act as major repositories for a wide range of contaminants 
including heavy metals, persistent organic pollutants, and industrial
chemicals, making their assessment critical for evaluating ecological 
risks and informing environmental management decisions
\cite{Chapman1990SedimentTriad, USGS_SedimentAssociatedContaminants_2019}.

Some popular evaluation methods include contaminant index-based approaches such as the Enrichment Factor (EF) and the
Geoaccumulation Index (Igeo).
These index-based approaches provide pollution scores relative to known background or reference values, 
often focusing on individual or selected chemical elements \cite{Birch2022Review} (Birch, 2022). 
For example, the Igeo index \cite{BIRCH2013282} (Birch, 2013) compares current concentrations to pre-industrial levels,
while the EF index compares current concentrations to regional background levels.
However, these approaches are limited by their reliance on accurate background 
concentration estimates and consideration of only a few chemical elements. 
When applied to datasets with many variables, they become less effective 
as they treat elements independently and fail to capture multivariate 
contamination patterns \cite{Grunfeld2005Outliers}.

A more general class of evaluation methods relies on multivariate ordination and data reduction techniques. 
These data-driven approaches consider all chemical variables simultaneously, revealing interaction patterns 
through dimensionality reduction without requiring background or reference values, thus avoiding the 
challenge of defining appropriate benchmarks \cite{Ciborowski2005ZoobenthicIndicators, Reynoldson1997ReferenceCondition}. 
However, such methods often yield composite axes or scores that lack intuitive interpretation and clear 
pollution thresholds, limiting their ability to provide comparable pollution categories like index-based 
methods \cite{Reimann2008Chemometrics}. This interpretability challenge has been recognized in ecological 
assessment contexts, where practitioners need actionable thresholds for management decisions 
\cite{Reynoldson1999RCA}.

Turn to large river and lake ecosystems, various pollution sources contribute to a complex array of contaminants,
escalating the need for approaches that can handle complex, multivariate contamination patterns.
Considering there are 30 stressor variables in my project and many of them share 
common pollution sources and interact via similar transport mechanisms,
a data-driven and multivariate approach is more suitable to capture the contamination levels and patterns.
Some ordination methods, such as Principal Component Analysis (PCA), can effectively reduce dimensionality and 
eliminate redundancy among correlated variables \cite{Zitko1994PCA}.
In addition to reducing dimensionality and eliminating redundancy,
advanced PCA methods can also enhance interpretability by incorporating variable weights \cite{Delchambre2015} (Delchambre, 2015) or spatial information \cite{Harris2011GWPCA} (Harris et al., 2011).,
making it possible to derive more meaningful contamination scores.



\subsection{Control of confounding environmental gradients}
In researching relationships between zoobenthic community composition and 
sediment contamination levels, naive associations can be confounded by environmental conditions
that are correlated with community composition or even contamination.
This induces omitted-variable bias and can either inflate or mask the true signal we seek 
for the zoobenthic condition index (ZCI). Accordingly, quantify the unique contribution of 
sediment contamination to community composition, after conditioning on measured environment,
is essential for deriving a reliable and interpretable ZCI.

To overcome this confounding issue, some common fixes include: \textit{(1) regression adjustment / partial regression; (2) variance partitioning; (3) blocking / stratification} \cite{Eberhardt1991FieldStudies, Wiens1995AccidentalImpacts}.
Approaches (1) and (2) are most often used in ecological regression as adjustments to the naïve model, whereas (3) is more common in experimental designs.
The third approach represents a design/estimator-based solution that mitigates latent confounding when environmental information is limited or unaddressed \cite{Byrnes2025CausalInference}.

Within the regression framework, adjustments are useful for statistically disentangling environmental influences \cite{Legendre2008VariationPartitioning}.
However, model performance can be sensitive to the sampling configuration and to the explanatory power of predictors \cite{Legendre2010Limitations,Fischer2018Uncertainty}.

Simulation studies show that model-revealed relationships depend not only on the true ecological relationships but also on the sampling--environmental distribution (SED), underscoring the importance of evenness and representativeness in sampling design \cite{Fischer2018Uncertainty}.
These limitations produce high uncertainty in estimates when any of the following occur: missing environmental predictors, poor coverage across environmental gradients, or insufficient sample size \cite{Eberhardt1991FieldStudies}.

Experimental designs can mitigate these issues when regression adjustment is not feasible, and analytical tools such as clustering or stratification can make the design more objective and data-driven \cite{Wiens1995AccidentalImpacts}.
However, these designs are also subject to SED and sample-size constraints.

A notable avenue is to infer \textit{counterfactual outcomes} for the response variable conditional on specified environmental gradients; these outcomes represent the environmentally deterministic component of the response \cite{Larsen2019CausalAnalysis, Byrnes2025CausalInference}.
This approach combines regression adjustment with experimental design: initial data for learning the counterfactual surface are selected via design (e.g., blocking/stratification), and the subsequent inference step uses trained regression models to estimate these deterministic counterfactual outcomes.


\subsection{Zoobenthic Community Composition Measurement}

Zoobenthic community composition measurement depends fundamentally on 
how we define and quantify the community, making it nearly impossible to establish a 
perfect measurement framework in real ecological systems \cite{Tampo2021Bioindicators}. 
Common descriptors of zoobenthic community composition include species richness, abundance, and biomass, 
which can be used individually or in combination to characterize the community \cite{Sitati2021AbundanceBiomass}. 
Each metric presents trade-offs between the amount of ecological information captured 
and the cost of data acquisition \cite{Menezes2010Trait}. 

The individual organism represents the minimal unit in a zoobenthic community and serves as 
the foundational unit for measuring abundance and biomass. 
While individual responses to external conditions ultimately comprise the community response,
their high variability and inherent stochasticity present significant challenges 
for using single or few individuals to adequately describe community-level responses.
In contrast, taxonomically rich assemblages can better represent community responses by 
averaging out individual-level variability and capturing broader ecological patterns \cite{Menezes2010Trait}.

Beyond the fundamental function of representing community structure, the choice of measurement approach 
depends critically on the study's primary objectives. In this project, the main purpose is to 
assess sediment contamination levels using zoobenthic community indicators \cite{Ciborowski2005ZoobenthicIndicators}.
Rooted in this purpose, we seek measurements that demonstrate sensitivity to contamination gradients—
whether reflecting raw stressor concentrations or evaluated contamination levels—such that 
pollution-sensitive metrics can outperform alternatives in mathematically predicting contamination levels.
However, such forecast-oriented measurements may exhibit limited mechanistic interpretability 
regarding the zoobenthic community composition itself, necessitating caution when 
interpreting results from a biological perspective.
To address this limitation, comprehensive mechanistic investigation of the taxa-based measurements 
and cross-validation with existing ecological knowledge becomes essential \cite{Reynoldson1999RCA}.


\subsection{Quantitative Regression Beyond the Mean and Threshold Detection}
Benefiting from the Central Limit Theorem and classical statistical theory,
mean-based regression has dominated ecological analysis \cite{Cade2003}.
Traditional linear regression performs adequately under weak correlations 
and homogeneous variance, but degrades substantially when these assumptions 
are violated—a common occurrence in ecology due to natural system complexity 
and unmeasurable latent factors. 
% While weighted least squares (WLS) addresses 
% heteroscedasticity through variance weighting, its linearity assumptions 
% still limit ecological applications.

Beyond statistical limitations, mean-based approaches can miss important ecological
patterns, elevating the meaningful investigation of relationships beyond the mean.
Ecological drivers often act in a heterogeneous way across the distribution of
responses, such that effects emerge only under extreme conditions and not in the 
center of the distribution. 
For instance, \cite{Schmidt2012Quantile}(Schmidt et al. 2012)
found that the influence of metal concentration was much stronger 
at higher quantiles of the biological response than on the mean.
Such upper‐tail–specific associations underscore the limitation of 
mean regression approaches when the primary ecological signal lies in extremes.
They thus motivate the use of “mean‐beyond” methods
- such as quantile regression - that can capture 
effects under high-stress or extreme conditions.

Nonlinear and threshold effects are another phenomenon observed in many ecological 
systems \cite{Cade2003} (Cade \& Noon, 2003). 
The concept of ecological thresholds emerged in the 1970s with the recognition 
that ecosystems often exhibit multiple 'stable' states under different conditions 
\cite{Holling1973} (Holling, 1973, as cited in Groffman et al., 2006).  
Peterson et al. \cite{peterson2006ecological} (Peterson et al., 2006) summarized 
three main applications of ecological thresholds: (1) \textbf{ecosystem state shifts} from small driver changes, 
(2) \textbf{critical loads} that ecosystems can absorb without functional changes, and 
(3) \textbf{extrinsic factor thresholds} where large-scale changes modify driver–response relationships.
These ecological recognitions motivate and justify the use of threshold-detecting 
methods in ecological regression modeling.

The threshold usually functions as a crucial decision point for management,
policy, or conservation actions.
Therefore, identifying and validating ecological thresholds is vital in both 
statistical and practical senses. 
Spake et al. \cite{Spake2022} emphasized the importance of \textit{scale}, 
highlighting four dimensions affecting threshold detection: (1) \textbf{grain} (measurement resolution), 
(2) \textbf{extent} (total area or duration covered), (3) \textbf{organizational level} (biological hierarchy), 
and (4) \textbf{analytical method} (statistical approach employed).
Turning to threshold detection, testing their existence and reliability is crucial.  
Daily et al. \cite{Daily2012} (Daily et al., 2012) showed that small sample sizes increase false detections, 
while larger samples improve reliability.  
The sample–environment distribution (SED) strongly influences detection and threshold location, 
with uneven SEDs producing bias.  
The rate of ecological change, whether linear or nonlinear, interacts with the chosen method, 
and user-defined parameters (e.g., quantile \(\tau\), bandwidth) play a critical role.